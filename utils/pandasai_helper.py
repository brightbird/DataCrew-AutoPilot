"""
PandasAIé›†æˆæ¨¡å— - ç”¨äºæ•°æ®å¯è§†åŒ–å’Œè¿›ä¸€æ­¥åˆ†æ
"""
import os
import pandas as pd
from typing import Optional, Union
import base64
from io import BytesIO
import sqlite3
import glob

# å°è¯•å¯¼å…¥PandasAIç›¸å…³æ¨¡å—
try:
    import pandasai as pai
    PANDASAI_AVAILABLE = True
    PANDASAI_ERROR = None
except ImportError as e:
    PANDASAI_AVAILABLE = False
    PANDASAI_ERROR = f"PandasAIæ ¸å¿ƒæ¨¡å—å¯¼å…¥å¤±è´¥: {e}"

try:
    from pandasai_openai.openai import OpenAI
    OPENAI_LLM_AVAILABLE = True
    OPENAI_LLM_ERROR = None
except ImportError as e:
    OPENAI_LLM_AVAILABLE = False
    OPENAI_LLM_ERROR = f"OpenAI LLMæ¨¡å—å¯¼å…¥å¤±è´¥: {e}"

class PandasAIAnalyzer:
    """PandasAIåˆ†æå™¨ç±»"""
    
    def __init__(self, db_path: str):
        """
        åˆå§‹åŒ–PandasAIåˆ†æå™¨
        
        Args:
            db_path: SQLiteæ•°æ®åº“è·¯å¾„
        """
        self.db_path = db_path
        self.analyzer_ready = False
        self.error_message = None
        
        # æ£€æŸ¥ä¾èµ–
        if not PANDASAI_AVAILABLE:
            raise ImportError(f"PandasAIä¸å¯ç”¨: {PANDASAI_ERROR}")
        
        if not OPENAI_LLM_AVAILABLE:
            raise ImportError(f"OpenAI LLMä¸å¯ç”¨: {OPENAI_LLM_ERROR}")
        
        try:
            self._setup_pandasai()
            self.analyzer_ready = True
        except Exception as e:
            self.error_message = str(e)
            raise e
    
    def _setup_pandasai(self):
        """è®¾ç½®PandasAIé…ç½®"""
        # ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„é…ç½®
        api_key = os.environ.get("OPENAI_API_KEY", os.environ.get("DASHSCOPE_API_KEY"))
        api_base = os.environ.get("OPENAI_API_BASE", "https://dashscope.aliyuncs.com/compatible-mode/v1")
        
        if not api_key:
            raise ValueError("æœªæ‰¾åˆ°APIå¯†é’¥ï¼Œè¯·è®¾ç½®DASHSCOPE_API_KEYæˆ–OPENAI_API_KEYç¯å¢ƒå˜é‡")
        
        # é…ç½®LLM - é‡ç‚¹ä¿®å¤modelå‚æ•°
        try:
            # ä¼˜å…ˆä½¿ç”¨é˜¿é‡Œäº‘ç™¾ç‚¼APIï¼ˆé¿å…åœ°åŒºé™åˆ¶é—®é¢˜ï¼‰
            if "dashscope" in api_base.lower():
                llm = OpenAI(
                    api_token=api_key,
                    base_url=api_base,
                    model="qwen-plus"  # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹åç§°
                )
                # æ˜ç¡®è®¾ç½®æ¨¡å‹åç§°
                llm.model = "qwen-plus"
            else:
                # ä½¿ç”¨æ ‡å‡†OpenAI API
                llm = OpenAI(
                    api_token=api_key,
                    model="gpt-3.5-turbo"  # ä½¿ç”¨æ›´ç¨³å®šçš„æ¨¡å‹
                )
                llm.model = "gpt-3.5-turbo"
                
        except Exception as e:
            print(f"LLMé…ç½®è­¦å‘Š: {e}")
            # ç®€åŒ–é…ç½®ä½œä¸ºåå¤‡
            llm = OpenAI(api_token=api_key)
            llm.model = "qwen-plus" if "dashscope" in api_base.lower() else "gpt-3.5-turbo"
        
        # é…ç½®PandasAI - æ·»åŠ æ›´å¤šé…ç½®é€‰é¡¹é¿å…é”™è¯¯
        try:
            pai.config.set({
                "llm": llm,
                "save_charts": True,
                "save_charts_path": "charts/",
                "verbose": False,  # å‡å°‘æ—¥å¿—è¾“å‡º
                "enable_cache": True,  # å¯ç”¨ç¼“å­˜
                "max_retries": 2,  # é™åˆ¶é‡è¯•æ¬¡æ•°
                "response_parser": "python"  # æ˜ç¡®æŒ‡å®šè§£æå™¨
            })
        except Exception as e:
            print(f"è­¦å‘Šï¼šPandasAIé…ç½®å¯èƒ½ä¸å®Œå…¨æˆåŠŸ: {e}")
            # åŸºæœ¬é…ç½®
            try:
                pai.config.set({
                    "llm": llm,
                    "verbose": False,
                    "max_retries": 1
                })
            except:
                # æœ€ç®€é…ç½®
                pai.config.llm = llm
    
    def query_to_dataframe(self, sql_query: str) -> pd.DataFrame:
        """
        æ‰§è¡ŒSQLæŸ¥è¯¢å¹¶è¿”å›pandas DataFrame
        
        Args:
            sql_query: SQLæŸ¥è¯¢è¯­å¥
            
        Returns:
            pandas DataFrame
        """
        try:
            conn = sqlite3.connect(self.db_path)
            df = pd.read_sql_query(sql_query, conn)
            conn.close()
            return df
        except Exception as e:
            raise Exception(f"SQLæŸ¥è¯¢æ‰§è¡Œå¤±è´¥: {e}")
    
    def analyze_with_natural_language(self, df: pd.DataFrame, question: str) -> str:
        """
        ä½¿ç”¨è‡ªç„¶è¯­è¨€åˆ†æDataFrame
        
        Args:
            df: pandas DataFrame
            question: è‡ªç„¶è¯­è¨€é—®é¢˜
            
        Returns:
            åˆ†æç»“æœ
        """
        try:
            # å°†DataFrameè½¬æ¢ä¸ºPandasAI DataFrame
            pai_df = pai.DataFrame(df)
            
            # æ·»åŠ æ˜ç¡®çš„æŒ‡å¯¼ï¼Œè¦æ±‚è¿”å›æ–‡æœ¬è€Œä¸æ˜¯ä»£ç 
            enhanced_question = f"""
            è¯·åˆ†ææ•°æ®å¹¶å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{question}
            
            è¦æ±‚ï¼š
            1. ç›´æ¥ç»™å‡ºåˆ†æç»“æœï¼Œä¸è¦ç”Ÿæˆä»£ç 
            2. ç”¨ä¸­æ–‡å›ç­”
            3. æä¾›å…·ä½“çš„æ•°å­—å’Œæ´å¯Ÿ
            """
            
            # ä½¿ç”¨è‡ªç„¶è¯­è¨€è¿›è¡Œåˆ†æ
            result = pai_df.chat(enhanced_question)
            
            return str(result) if result else "åˆ†æå®Œæˆï¼Œä½†æ²¡æœ‰ç”Ÿæˆå…·ä½“ç»“æœã€‚"
            
        except Exception as e:
            # æä¾›é™çº§åˆ†æ
            return self._provide_basic_analysis(df, question, str(e))
    
    def _provide_basic_analysis(self, df: pd.DataFrame, question: str, error_msg: str) -> str:
        """
        æä¾›åŸºç¡€æ•°æ®åˆ†æï¼ˆå½“PandasAIå¤±è´¥æ—¶ï¼‰
        
        Args:
            df: pandas DataFrame
            question: ç”¨æˆ·é—®é¢˜
            error_msg: é”™è¯¯ä¿¡æ¯
            
        Returns:
            åŸºç¡€åˆ†æç»“æœ
        """
        try:
            analysis = []
            analysis.append(f"ğŸ“Š æ•°æ®åŸºæœ¬ä¿¡æ¯ï¼š")
            analysis.append(f"- æ•°æ®è¡Œæ•°ï¼š{len(df)}")
            analysis.append(f"- æ•°æ®åˆ—æ•°ï¼š{len(df.columns)}")
            analysis.append(f"- åˆ—åï¼š{', '.join(df.columns.tolist())}")
            
            # æ•°å€¼åˆ—åˆ†æ
            numeric_cols = df.select_dtypes(include=['number']).columns
            if len(numeric_cols) > 0:
                analysis.append(f"\nğŸ“ˆ æ•°å€¼åˆ—ç»Ÿè®¡ï¼š")
                for col in numeric_cols[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªæ•°å€¼åˆ—
                    try:
                        stats = df[col].describe()
                        analysis.append(f"- {col}ï¼šå¹³å‡å€¼ {stats['mean']:.2f}ï¼Œæœ€å¤§å€¼ {stats['max']:.2f}ï¼Œæœ€å°å€¼ {stats['min']:.2f}")
                    except:
                        pass
            
            # æ–‡æœ¬åˆ—åˆ†æ
            text_cols = df.select_dtypes(include=['object']).columns
            if len(text_cols) > 0:
                analysis.append(f"\nğŸ“ æ–‡æœ¬åˆ—ä¿¡æ¯ï¼š")
                for col in text_cols[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªæ–‡æœ¬åˆ—
                    try:
                        unique_count = df[col].nunique()
                        analysis.append(f"- {col}ï¼š{unique_count} ä¸ªä¸åŒå€¼")
                    except:
                        pass
            
            # æ ¹æ®é—®é¢˜ç±»å‹æä¾›ç›¸å…³åˆ†æ
            question_lower = question.lower()
            if "æœ€é«˜" in question_lower or "æœ€å¤§" in question_lower:
                if len(numeric_cols) > 0:
                    col = numeric_cols[0]
                    max_row = df.loc[df[col].idxmax()]
                    analysis.append(f"\nğŸ† {col}çš„æœ€å¤§å€¼ï¼š{max_row[col]}")
            elif "æœ€ä½" in question_lower or "æœ€å°" in question_lower:
                if len(numeric_cols) > 0:
                    col = numeric_cols[0]
                    min_row = df.loc[df[col].idxmin()]
                    analysis.append(f"\nğŸ“‰ {col}çš„æœ€å°å€¼ï¼š{min_row[col]}")
            elif "è¶‹åŠ¿" in question_lower:
                analysis.append(f"\nğŸ“Š è¶‹åŠ¿åˆ†æéœ€è¦æ—¶é—´åºåˆ—æ•°æ®ï¼Œå½“å‰æ•°æ®åŒ…å« {len(df)} ä¸ªæ•°æ®ç‚¹")
            
            analysis.append(f"\nâš ï¸ æ³¨æ„ï¼šç”±äºPandasAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼ˆ{error_msg[:100]}...ï¼‰ï¼Œä»¥ä¸Šæ˜¯åŸºç¡€ç»Ÿè®¡åˆ†æã€‚")
            
            return "\n".join(analysis)
            
        except Exception as e:
            return f"æ•°æ®åˆ†æå¤±è´¥ï¼š{e}ã€‚æ•°æ®åŒ…å« {len(df)} è¡Œ {len(df.columns)} åˆ—ã€‚"
    
    def create_visualization(self, df: pd.DataFrame, chart_request: str) -> Optional[dict]:
        """
        åˆ›å»ºæ•°æ®å¯è§†åŒ–
        
        Args:
            df: pandas DataFrame
            chart_request: å›¾è¡¨è¯·æ±‚ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰
            
        Returns:
            åŒ…å«å›¾è¡¨ä¿¡æ¯çš„å­—å…¸ï¼Œæ ¼å¼ï¼š
            {
                "type": "image",
                "path": "å›¾ç‰‡æ–‡ä»¶è·¯å¾„",
                "base64": "base64ç¼–ç çš„å›¾ç‰‡æ•°æ®",
                "message": "æç¤ºä¿¡æ¯"
            }
            æˆ–è€…
            {
                "type": "text",
                "content": "æ–‡æœ¬ç»“æœ",
                "message": "æç¤ºä¿¡æ¯"
            }
        """
        try:
            # ç¡®ä¿chartsç›®å½•å­˜åœ¨
            charts_dir = "charts"
            exports_charts_dir = "exports/charts"  # PandasAIé»˜è®¤ç›®å½•
            
            for dir_path in [charts_dir, exports_charts_dir]:
                if not os.path.exists(dir_path):
                    os.makedirs(dir_path)
            
            # è®°å½•ç”Ÿæˆå›¾è¡¨å‰çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆæ£€æŸ¥ä¸¤ä¸ªç›®å½•ï¼‰
            existing_files = set()
            for pattern in ["charts/*.png", "charts/*.jpg", "charts/*.jpeg",
                           "exports/charts/*.png", "exports/charts/*.jpg", "exports/charts/*.jpeg"]:
                existing_files.update(glob.glob(pattern))
            
            # å°†DataFrameè½¬æ¢ä¸ºPandasAI DataFrame
            pai_df = pai.DataFrame(df)
            
            # åˆ›å»ºå¯è§†åŒ–
            result = pai_df.chat(chart_request)
            
            # æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†æ–°çš„å›¾ç‰‡æ–‡ä»¶
            new_files = set()
            for pattern in ["charts/*.png", "charts/*.jpg", "charts/*.jpeg",
                           "exports/charts/*.png", "exports/charts/*.jpg", "exports/charts/*.jpeg"]:
                new_files.update(glob.glob(pattern))
            
            # æ‰¾åˆ°æ–°ç”Ÿæˆçš„æ–‡ä»¶
            new_chart_files = new_files - existing_files
            
            if new_chart_files:
                # å–æœ€æ–°çš„æ–‡ä»¶
                latest_chart = max(new_chart_files, key=os.path.getctime)
                
                # è¯»å–å¹¶è½¬æ¢ä¸ºbase64
                with open(latest_chart, 'rb') as f:
                    img_data = f.read()
                base64_img = base64.b64encode(img_data).decode()
                
                return {
                    "type": "image",
                    "path": latest_chart,
                    "base64": base64_img,
                    "message": f"å›¾è¡¨å·²ç”Ÿæˆï¼š{os.path.basename(latest_chart)}"
                }
            
            # å¦‚æœç»“æœæ˜¯æ–‡ä»¶è·¯å¾„å­—ç¬¦ä¸²
            elif isinstance(result, str):
                # æ£€æŸ¥æ˜¯å¦æ˜¯å›¾ç‰‡æ–‡ä»¶è·¯å¾„
                if result.endswith(('.png', '.jpg', '.jpeg')):
                    if os.path.exists(result):
                        with open(result, 'rb') as f:
                            img_data = f.read()
                        base64_img = base64.b64encode(img_data).decode()
                        
                        return {
                            "type": "image",
                            "path": result,
                            "base64": base64_img,
                            "message": f"å›¾è¡¨å·²ç”Ÿæˆï¼š{os.path.basename(result)}"
                        }
                    else:
                        return {
                            "type": "text",
                            "content": result,
                            "message": "PandasAIè¿”å›äº†æ–‡ä»¶è·¯å¾„ï¼Œä½†æ–‡ä»¶ä¸å­˜åœ¨"
                        }
                else:
                    # æ–‡æœ¬ç»“æœ
                    return {
                        "type": "text",
                        "content": str(result),
                        "message": "PandasAIè¿”å›äº†æ–‡æœ¬ç»“æœè€Œä¸æ˜¯å›¾è¡¨"
                    }
            
            # å…¶ä»–ç±»å‹çš„ç»“æœ
            else:
                return {
                    "type": "text",
                    "content": str(result) if result else "æœªç”Ÿæˆä»»ä½•å†…å®¹",
                    "message": "PandasAIæ²¡æœ‰ç”Ÿæˆå›¾è¡¨ï¼Œè¿”å›äº†å…¶ä»–ç±»å‹çš„ç»“æœ"
                }
                
        except Exception as e:
            print(f"å¯è§†åŒ–åˆ›å»ºå¤±è´¥: {e}")
            return {
                "type": "error",
                "content": f"å¯è§†åŒ–åˆ›å»ºå¤±è´¥: {e}",
                "message": "å›¾è¡¨ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯"
            }
    
    def get_data_insights(self, df: pd.DataFrame) -> str:
        """
        è·å–æ•°æ®æ´å¯Ÿ
        
        Args:
            df: pandas DataFrame
            
        Returns:
            æ•°æ®æ´å¯Ÿæ–‡æœ¬
        """
        try:
            pai_df = pai.DataFrame(df)
            
            # è·å–åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            insights_prompt = """
            è¯·ä¸ºè¿™ä¸ªæ•°æ®é›†æä¾›è¯¦ç»†çš„æ•°æ®æ´å¯Ÿåˆ†æã€‚è¦æ±‚ï¼š
            1. ç”¨ä¸­æ–‡å›ç­”
            2. ä¸è¦ç”Ÿæˆä»£ç ï¼Œç›´æ¥ç»™å‡ºåˆ†æç»“æœ
            3. åŒ…æ‹¬ä¸»è¦ç»Ÿè®¡ä¿¡æ¯ã€æ•°æ®åˆ†å¸ƒã€å¼‚å¸¸å€¼ã€è¶‹åŠ¿ç­‰
            4. æä¾›å…·ä½“çš„æ•°å­—å’Œç™¾åˆ†æ¯”
            """
            
            insights = pai_df.chat(insights_prompt)
            
            return str(insights) if insights else self._generate_basic_insights(df)
            
        except Exception as e:
            print(f"PandasAIæ´å¯Ÿç”Ÿæˆå¤±è´¥: {e}")
            return self._generate_basic_insights(df)
    
    def _generate_basic_insights(self, df: pd.DataFrame) -> str:
        """
        ç”ŸæˆåŸºç¡€æ•°æ®æ´å¯Ÿï¼ˆå½“PandasAIå¤±è´¥æ—¶ï¼‰
        
        Args:
            df: pandas DataFrame
            
        Returns:
            åŸºç¡€æ´å¯Ÿæ–‡æœ¬
        """
        try:
            insights = []
            insights.append("ğŸ“Š **æ•°æ®æ´å¯ŸæŠ¥å‘Š**\n")
            
            # åŸºæœ¬ä¿¡æ¯
            insights.append(f"**æ•°æ®æ¦‚è§ˆï¼š**")
            insights.append(f"- æ€»è®°å½•æ•°ï¼š{len(df):,} è¡Œ")
            insights.append(f"- å­—æ®µæ•°é‡ï¼š{len(df.columns)} ä¸ª")
            insights.append(f"- å†…å­˜ä½¿ç”¨ï¼š{df.memory_usage(deep=True).sum() / 1024:.1f} KB\n")
            
            # æ•°æ®ç±»å‹åˆ†æ
            numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
            text_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
            date_cols = df.select_dtypes(include=['datetime']).columns.tolist()
            
            insights.append(f"**æ•°æ®ç±»å‹åˆ†å¸ƒï¼š**")
            insights.append(f"- æ•°å€¼å­—æ®µï¼š{len(numeric_cols)} ä¸ª ({', '.join(numeric_cols[:3])}{'...' if len(numeric_cols) > 3 else ''})")
            insights.append(f"- æ–‡æœ¬å­—æ®µï¼š{len(text_cols)} ä¸ª ({', '.join(text_cols[:3])}{'...' if len(text_cols) > 3 else ''})")
            if date_cols:
                insights.append(f"- æ—¥æœŸå­—æ®µï¼š{len(date_cols)} ä¸ª ({', '.join(date_cols)})")
            insights.append("")
            
            # æ•°å€¼å­—æ®µç»Ÿè®¡
            if numeric_cols:
                insights.append(f"**æ•°å€¼å­—æ®µç»Ÿè®¡ï¼š**")
                for col in numeric_cols[:3]:  # åªåˆ†æå‰3ä¸ªæ•°å€¼å­—æ®µ
                    try:
                        stats = df[col].describe()
                        null_pct = (df[col].isnull().sum() / len(df)) * 100
                        insights.append(f"- **{col}**ï¼š")
                        insights.append(f"  - å¹³å‡å€¼ï¼š{stats['mean']:.2f}")
                        insights.append(f"  - ä¸­ä½æ•°ï¼š{stats['50%']:.2f}")
                        insights.append(f"  - æ ‡å‡†å·®ï¼š{stats['std']:.2f}")
                        insights.append(f"  - èŒƒå›´ï¼š{stats['min']:.2f} ~ {stats['max']:.2f}")
                        if null_pct > 0:
                            insights.append(f"  - ç¼ºå¤±å€¼ï¼š{null_pct:.1f}%")
                    except:
                        insights.append(f"- **{col}**ï¼šç»Ÿè®¡åˆ†æå¤±è´¥")
                insights.append("")
            
            # æ–‡æœ¬å­—æ®µåˆ†æ
            if text_cols:
                insights.append(f"**åˆ†ç±»å­—æ®µåˆ†æï¼š**")
                for col in text_cols[:3]:  # åªåˆ†æå‰3ä¸ªæ–‡æœ¬å­—æ®µ
                    try:
                        unique_count = df[col].nunique()
                        null_count = df[col].isnull().sum()
                        most_common = df[col].value_counts().head(1)
                        insights.append(f"- **{col}**ï¼š")
                        insights.append(f"  - å”¯ä¸€å€¼æ•°é‡ï¼š{unique_count}")
                        insights.append(f"  - ç¼ºå¤±å€¼ï¼š{null_count}")
                        if len(most_common) > 0:
                            insights.append(f"  - æœ€å¸¸è§å€¼ï¼š{most_common.index[0]} ({most_common.iloc[0]} æ¬¡)")
                    except:
                        insights.append(f"- **{col}**ï¼šåˆ†æå¤±è´¥")
                insights.append("")
            
            # æ•°æ®è´¨é‡è¯„ä¼°
            insights.append(f"**æ•°æ®è´¨é‡è¯„ä¼°ï¼š**")
            total_cells = len(df) * len(df.columns)
            null_cells = df.isnull().sum().sum()
            completeness = ((total_cells - null_cells) / total_cells) * 100
            insights.append(f"- æ•°æ®å®Œæ•´æ€§ï¼š{completeness:.1f}%")
            
            if null_cells > 0:
                insights.append(f"- ç¼ºå¤±å€¼æ€»æ•°ï¼š{null_cells:,} ä¸ª")
                null_cols = df.columns[df.isnull().any()].tolist()
                insights.append(f"- æœ‰ç¼ºå¤±å€¼çš„å­—æ®µï¼š{', '.join(null_cols[:5])}{'...' if len(null_cols) > 5 else ''}")
            
            # å¼‚å¸¸å€¼æ£€æµ‹ï¼ˆç®€å•ç‰ˆæœ¬ï¼‰
            if numeric_cols:
                insights.append("")
                insights.append(f"**å¼‚å¸¸å€¼æ£€æµ‹ï¼š**")
                for col in numeric_cols[:2]:  # åªæ£€æµ‹å‰2ä¸ªæ•°å€¼å­—æ®µ
                    try:
                        Q1 = df[col].quantile(0.25)
                        Q3 = df[col].quantile(0.75)
                        IQR = Q3 - Q1
                        outliers = df[(df[col] < Q1 - 1.5 * IQR) | (df[col] > Q3 + 1.5 * IQR)]
                        outlier_pct = (len(outliers) / len(df)) * 100
                        insights.append(f"- **{col}**ï¼š{len(outliers)} ä¸ªå¼‚å¸¸å€¼ ({outlier_pct:.1f}%)")
                    except:
                        pass
            
            insights.append("\nğŸ’¡ **å»ºè®®ï¼š** è¿™æ˜¯åŸºç¡€ç»Ÿè®¡åˆ†æã€‚è¦è·å¾—æ›´æ·±å…¥çš„æ´å¯Ÿï¼Œå»ºè®®ä½¿ç”¨ä¸“ä¸šçš„æ•°æ®åˆ†æå·¥å…·ã€‚")
            
            return "\n".join(insights)
            
        except Exception as e:
            return f"æ— æ³•ç”Ÿæˆæ•°æ®æ´å¯Ÿï¼š{e}"
    
    def suggest_next_questions(self, df: pd.DataFrame, current_query: str) -> list:
        """
        åŸºäºå½“å‰æ•°æ®å’ŒæŸ¥è¯¢å»ºè®®ä¸‹ä¸€æ­¥é—®é¢˜
        
        Args:
            df: pandas DataFrame
            current_query: å½“å‰æŸ¥è¯¢
            
        Returns:
            å»ºè®®é—®é¢˜åˆ—è¡¨
        """
        try:
            # é¦–å…ˆå°è¯•ä½¿ç”¨PandasAIç”Ÿæˆå»ºè®®
            pai_df = pai.DataFrame(df)
            
            suggestion_prompt = f"""
            åŸºäºè¿™ä¸ªæ•°æ®é›†å’Œå½“å‰æŸ¥è¯¢ï¼š'{current_query}'ï¼Œ
            è¯·å»ºè®®5ä¸ªç›¸å…³çš„åç»­åˆ†æé—®é¢˜ã€‚è¯·ç›´æ¥è¿”å›é—®é¢˜åˆ—è¡¨ï¼Œä¸è¦ç”Ÿæˆä»£ç ã€‚
            æ ¼å¼è¦æ±‚ï¼š
            1. é—®é¢˜1
            2. é—®é¢˜2
            3. é—®é¢˜3
            4. é—®é¢˜4
            5. é—®é¢˜5
            """
            
            # è®¾ç½®è¶…æ—¶å’Œé‡è¯•é™åˆ¶
            suggestions = pai_df.chat(suggestion_prompt)
            
            # è§£æå»ºè®®
            if isinstance(suggestions, str):
                lines = suggestions.strip().split('\n')
                questions = []
                for line in lines:
                    line = line.strip()
                    if line and any(line.startswith(str(i)) for i in range(1, 10)):
                        # ç§»é™¤åºå·
                        question = line.split('.', 1)[1].strip() if '.' in line else line
                        if question:  # ç¡®ä¿é—®é¢˜ä¸ä¸ºç©º
                            questions.append(question)
                
                if questions:
                    return questions[:5]  # æœ€å¤šè¿”å›5ä¸ªå»ºè®®
            
            # å¦‚æœPandasAIå¤±è´¥ï¼Œä½¿ç”¨é¢„å®šä¹‰çš„æ™ºèƒ½å»ºè®®
            return self._generate_fallback_suggestions(df, current_query)
            
        except Exception as e:
            print(f"PandasAIå»ºè®®ç”Ÿæˆå¤±è´¥: {e}")
            # ä½¿ç”¨é™çº§æ–¹æ¡ˆ
            return self._generate_fallback_suggestions(df, current_query)
    
    def _generate_fallback_suggestions(self, df: pd.DataFrame, current_query: str) -> list:
        """
        ç”Ÿæˆé™çº§å»ºè®®é—®é¢˜ï¼ˆå½“PandasAIä¸å¯ç”¨æ—¶ï¼‰
        
        Args:
            df: pandas DataFrame
            current_query: å½“å‰æŸ¥è¯¢
            
        Returns:
            å»ºè®®é—®é¢˜åˆ—è¡¨
        """
        # åˆ†ææ•°æ®ç‰¹å¾
        numeric_columns = df.select_dtypes(include=['number']).columns.tolist()
        categorical_columns = df.select_dtypes(include=['object', 'category']).columns.tolist()
        
        suggestions = []
        
        # åŸºäºæ•°æ®ç‰¹å¾ç”Ÿæˆå»ºè®®
        if numeric_columns:
            if len(numeric_columns) >= 2:
                suggestions.append(f"åˆ†æ{numeric_columns[0]}å’Œ{numeric_columns[1]}ä¹‹é—´çš„ç›¸å…³æ€§")
            suggestions.append(f"è®¡ç®—{numeric_columns[0]}çš„ç»Ÿè®¡åˆ†å¸ƒæƒ…å†µ")
            suggestions.append(f"è¯†åˆ«{numeric_columns[0]}ä¸­çš„å¼‚å¸¸å€¼æˆ–æå€¼")
        
        if categorical_columns:
            suggestions.append(f"æŒ‰{categorical_columns[0]}è¿›è¡Œåˆ†ç»„åˆ†æ")
            if len(categorical_columns) >= 2:
                suggestions.append(f"å¯¹æ¯”ä¸åŒ{categorical_columns[0]}å’Œ{categorical_columns[1]}çš„è¡¨ç°")
        
        # åŸºäºå½“å‰æŸ¥è¯¢å†…å®¹ç”Ÿæˆç›¸å…³å»ºè®®
        query_lower = current_query.lower()
        if "é”€å”®" in query_lower or "æ”¶å…¥" in query_lower or "revenue" in query_lower:
            suggestions.extend([
                "åˆ†æé”€å”®è¶‹åŠ¿çš„å­£èŠ‚æ€§å˜åŒ–",
                "è®¡ç®—å„äº§å“çš„å¸‚åœºä»½é¢å æ¯”",
                "è¯†åˆ«é”€å”®å¢é•¿æœ€å¿«çš„äº§å“ç±»åˆ«",
                "åˆ†æä»·æ ¼ä¸é”€é‡çš„å…³ç³»",
                "å¯¹æ¯”ä¸åŒæ—¶é—´æ®µçš„é”€å”®è¡¨ç°"
            ])
        elif "äº§å“" in query_lower or "product" in query_lower:
            suggestions.extend([
                "åˆ†æäº§å“çš„ç”Ÿå‘½å‘¨æœŸé˜¶æ®µ",
                "è®¡ç®—äº§å“çš„å¹³å‡å”®ä»·å˜åŒ–",
                "è¯†åˆ«æœ€å—æ¬¢è¿çš„äº§å“ç‰¹å¾",
                "åˆ†æäº§å“çš„åº“å­˜å‘¨è½¬ç‡",
                "å¯¹æ¯”ä¸åŒäº§å“ç±»åˆ«çš„ç›ˆåˆ©èƒ½åŠ›"
            ])
        elif "å®¢æˆ·" in query_lower or "customer" in query_lower:
            suggestions.extend([
                "åˆ†æå®¢æˆ·çš„è´­ä¹°è¡Œä¸ºæ¨¡å¼",
                "è®¡ç®—å®¢æˆ·çš„ç”Ÿå‘½å‘¨æœŸä»·å€¼",
                "è¯†åˆ«é«˜ä»·å€¼å®¢æˆ·ç¾¤ä½“",
                "åˆ†æå®¢æˆ·æµå¤±çš„ä¸»è¦åŸå› ",
                "å¯¹æ¯”ä¸åŒå®¢æˆ·ç¾¤ä½“çš„æ¶ˆè´¹ä¹ æƒ¯"
            ])
        else:
            # é€šç”¨å»ºè®®
            suggestions.extend([
                "è¿›è¡Œæ•°æ®çš„è¶‹åŠ¿åˆ†æ",
                "è®¡ç®—å…³é”®æŒ‡æ ‡çš„å˜åŒ–ç‡",
                "è¯†åˆ«æ•°æ®ä¸­çš„å¼‚å¸¸æ¨¡å¼",
                "è¿›è¡Œåˆ†ç»„å¯¹æ¯”åˆ†æ",
                "åˆ†ææ•°æ®çš„åˆ†å¸ƒç‰¹å¾"
            ])
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_suggestions = list(dict.fromkeys(suggestions))  # ä¿æŒé¡ºåºçš„å»é‡
        return unique_suggestions[:5]
    
    def compare_data_trends(self, df: pd.DataFrame, comparison_request: str) -> str:
        """
        æ¯”è¾ƒæ•°æ®è¶‹åŠ¿
        
        Args:
            df: pandas DataFrame
            comparison_request: æ¯”è¾ƒè¯·æ±‚
            
        Returns:
            æ¯”è¾ƒç»“æœ
        """
        try:
            pai_df = pai.DataFrame(df)
            result = pai_df.chat(comparison_request)
            return str(result)
        except Exception as e:
            return f"è¶‹åŠ¿æ¯”è¾ƒå¤±è´¥: {e}" 